> タイトル<br>
Long Range Arena : A Benchmark for Efficient Transformers  
高効率トランスのベンチマーク
<br>

記入欄
***

> 論文リンク<br>
https://openreview.net/forum?id=qVyeW-grC2k
<br>

記入欄
***

> 要約メモ<br>
Transformersは、主に2次の自己注意の複雑さのために、長いシーケンス長にはあまりうまく対応できません。ここ数ヶ月の間に、この問題に対処するために、効率的で高速なTransformerが幅広く提案されており、多くの場合、vanilla Transformerモデルよりも優れた、または同等のモデル品質を主張しています。今日まで、このクラスのモデルをどのように評価するかについて、確立されたコンセンサスはありません。さらに、様々なタスクやデータセットで一貫性のないベンチマークが行われているため、多くのモデルの中から相対的なモデル品質を評価することが困難になっています。本論文では、長い文脈のシナリオでモデルの品質を評価することに特化した、体系的で統一されたベンチマーク「Long Range Arena」を提案します。本ベンチマークでは、テキスト、自然画像、合成画像、類似性、構造、視覚的・空間的推論を必要とする数式など、幅広いデータタイプとモダリティを網羅した、トークンからトークンまでのシーケンスからなる一連のタスクを対象とする。我々は、新たに提案したベンチマーク群において、10種類の確立された長距離トランスフォーマーモデル（Reformers、Linformers、Linear Transformers、Sinkhorn Transformers、Performers、Synthesizer、Sparse Transformers、Longformers）を系統的に評価した。Long Range Arena」は、このクラスの効率的なトランスフォーマーモデルをよりよく理解するための道を開き、この方向での研究を促進し、取り組むべき新たな課題を提示しています。
<br>

記入欄
***

> どんなもの？<br>

<br>

記入欄
***

> 先行文献と比べてどこがすごい？

<br>

記入欄
***

> 技術や手法のキモは？

<br>

記入欄
***

> どうやって有効性を証明した？

<br>

記入欄
***

> 議論はある？

<br>

記入欄
***

> 分からない単語、調べた内容メモ

<br>

記入欄
***

> 参考リンク

<br>

記入欄
***