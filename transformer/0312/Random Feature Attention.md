> タイトル<br>
Random Feature Attention
<br>

記入欄
***

> 論文リンク<br>
https://openreview.net/forum?id=QtTKTdVrFBB
<br>

記入欄
***

> 要約メモ<br>
トランスフォーマーは、様々なシーケンスモデリングタスクに対応する最先端のモデルです。トランスフォーマーの核となるのは、タイムステップごとに入力間のペアワイズ相互作用をモデル化するアテンション関数です。アテンションは強力ですが、シーケンスの長さに対して時間と空間が2次的に複雑になるため、長いシーケンスに対しては効率的に拡張できません。本研究では、ランダム特徴法を用いてソフトマックス関数を近似する、時間と空間が線形のアテンションであるRFAを提案し、トランスフォーマーへの応用を検討する。RFAは、従来のソフトマックスアテンションの代替として使用することができ、オプションのゲーティングメカニズムによって再帰性バイアスを考慮した学習を簡単に行うことができる。言語モデリングと機械翻訳の実験では、RFAが強力な変換器のベースラインと比較して、同等以上の性能を達成することが実証された。機械翻訳の実験では、RFAはバニラ変換器の2倍の速度でデコードしています。既存の効率的な変換器の改良型と比較して、RFAは3つの長文分類データセットにおいて、精度と効率の両方の点で競争力がある。我々の分析によると，RFAの効率性の向上は，特に長いシーケンスにおいて顕著であり，RFAは，大規模な入力，高速なデコーディング，低メモリフットプリントを必要とするタスクにおいて特に有用であることが示唆される．
一言要約：我々は、シーケンスの長さに応じて線形にスケールするランダムフィーチャベースのアテンションを提案し、言語モデリングと機械翻訳において強力な変換器のベースラインと同等の性能を発揮する。
<br>

記入欄
***

> どんなもの？<br>

<br>

記入欄
***

> 先行文献と比べてどこがすごい？

<br>

記入欄
***

> 技術や手法のキモは？

<br>

記入欄
***

> どうやって有効性を証明した？

<br>

記入欄
***

> 議論はある？

<br>

記入欄
***

> 分からない単語、調べた内容メモ

<br>

記入欄
***

> 参考リンク

<br>

記入欄
***