> タイトル<br> 
Pre-training Text-to-Text Transformers for Concept-centric Common Sense  
概念中心のコモンセンスのためのText-to-Text Transformersの事前学習
<br>

記入欄
***

> 論文リンク<br>
https://openreview.net/forum?id=3k20LAiHYL2
<br>

記入欄
***

> 要約メモ<br>
事前学習された言語モデル（PTLM）は、テキストの構文的・意味的な理解を必要とする様々な自然言語理解（NLU）や生成（NLG）のタスクにおいて、素晴らしい結果を達成しています。しかし、BERTスタイルのPTLMのマスク付きトークン予測や、T5スタイルのPTLMのマスク付きスパン・インフィリングなどの現在の事前学習目的では、日常的な概念に関する関係的・構成的なコモンセンス知識を明示的にモデル化することができません。これは、コモンセンス推論を必要とする多くの下流タスクにとって極めて重要です。PTLMを常識的に拡張するために、我々は一般的な事前学習と下流のタスクに特化した微調整の間の中間的な自己教師付き事前学習タスクとして、生成的および対照的な目的を提案する。また、生成的目的と対比的目的を統合し、これらの目的がより効果的になるような共同学習フレームワークを提案する。
我々の提案する目的は、外部の知識ベースに頼ることなく、事前に訓練されたテキストからテキストへの変換器のパラメータに、より多くの常識的な知識を詰め込むことができ、NLUとNLGの両方のタスクでより良い性能を得ることができる。本研究では、概念認識言語モデル（CALM）を学習するために、事前に学習したT5モデルに本手法を適用し、5つのコモンセンスベンチマーク（4つのNLUタスクと1つのNLGタスク）で実験を行った。実験の結果、CALMはベースラインの手法を一定のマージンで上回ることがわかった。
<br>

記入欄
***

> どんなもの？<br>

<br>

記入欄
***

> 先行文献と比べてどこがすごい？

<br>

記入欄
***

> 技術や手法のキモは？

<br>

記入欄
***

> どうやって有効性を証明した？

<br>

記入欄
***

> 議論はある？

<br>

記入欄
***

> 分からない単語、調べた内容メモ

<br>

記入欄
***

> 参考リンク

<br>

記入欄
***