> タイトル<br>
IOT: Instance-wise Layer Reordering for Transformer Structures
<br>

記入欄
***

> 論文リンク<br>
https://openreview.net/forum?id=ipUPfYxWZvM
<br>

記入欄
***

> 要約メモ<br>
Transformerは、自己注意、（オプションの）エンコーダ・デコーダ注意、フィードフォワード層を順次積み重ねることで、自然言語処理（NLP）において大きな成功を収めており、多くのバリエーションが提案されています。現在のところ、これらのモデルのほとんどは、データサンプルが異なっていても、layer orderが固定されていて同じであると仮定しています。しかし、実際にはデータサンプルによって異なる層の順序が好まれることがわかっています。  
この観察に基づいて、本研究では、Transformerにおける固定された層の順序の仮定を破り、モデル構造にインスタンス単位での層の順序変更を導入します。本研究のInstance-wise Ordered Transformer (IOT)は、並び替えられた層によって変種機能をモデル化することができ、各サンプルがより良いものを選択することで、ほぼ同じ数のパラメータという制約の下でモデルの性能を向上させることができます。これを実現するために、パラメータと推論コストが無視できる軽い予測器を導入し、任意の入力シーケンスに対して最も有能で有利な層の順序を決定します。タスク（ニューラル機械翻訳、抽象的な要約、コード生成）とデータセットを用いた実験により、本手法が一貫して改善されていることを実証する。さらに、本手法は、Transformer以外の他のアーキテクチャにも適用できることを示しています。本研究のコードはGithubで公開されています。
<br>

記入欄
***

> どんなもの？<br>

<br>

記入欄
***

> 先行文献と比べてどこがすごい？

<br>

記入欄
***

> 技術や手法のキモは？

<br>

記入欄
***

> どうやって有効性を証明した？

<br>

記入欄
***

> 議論はある？

<br>

記入欄
***

> 分からない単語、調べた内容メモ

<br>

記入欄
***

> 参考リンク

<br>

記入欄
***