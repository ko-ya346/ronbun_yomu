> タイトル<br>
HyperGrid Transformers: Towards A Single Model for Multiple Tasks
1つのモデルで複数のタスクをこなすことを目指して
<br>

記入欄
***

> 論文リンク<br>
https://openreview.net/forum?id=hiq1rHO8pNT
<br>

記入欄
***

> 要約メモ<br>
自然言語理解タスクにおいて最先端の性能を達成するためには、通常、タスクごとに新しいモデルを微調整する必要があります。この方法では、複数のモデルを提供するための技術的なメンテナンスが必要になるだけでなく、全体的なパラメータコストも高くなります。すべてのタスクに対応できる単一のマルチタスクモデルを学習することは、困難ではあるが魅力的な提案である。本論文では、フィードフォワード層を制御するためにタスクコンディショニングされたハイパーネットワークを活用する新しいトランスフォーマーアーキテクチャであるHyperGrid Transformersを提案します。具体的には、異なるタスクのために重み行列の領域を特化するのに役立つグリッド単位の投影を学習する、分解可能なハイパーネットワークを提案する。提案するハイパーネットワークを構築するために、我々の手法は、グローバルな（タスクにとらわれない）状態とローカルなタスク固有の状態の間の相互作用と構成を学習する。我々はGLUE/SuperGLUEを用いて広範な実験を行った。SuperGLUEテストセットでは、パラメータ効率が数倍高いにもかかわらず、最先端技術と同等の性能を発揮しました。この手法は、ファインチューニングとマルチタスク学習アプローチの間のギャップを埋めるのに役立ちます。
<br>

記入欄
***

> どんなもの？<br>

<br>

記入欄
***

> 先行文献と比べてどこがすごい？

<br>

記入欄
***

> 技術や手法のキモは？

<br>

記入欄
***

> どうやって有効性を証明した？

<br>

記入欄
***

> 議論はある？

<br>

記入欄
***

> 分からない単語、調べた内容メモ

<br>

記入欄
***

> 参考リンク

<br>

記入欄
***