> タイトル<br>
Parameter Efficient Multimodal Transformers for Video Representation Learning  
映像表現学習のためのパラメータ効率の良いマルチモーダル・トランスフォーマー
<br>

記入欄
***

> 論文リンク<br>
https://openreview.net/forum?id=6UdQLhqJyFD
<br>

記入欄
***

> 要約メモ<br>
近年、Transformerは言語分野で成功を収めており、これをマルチモーダル環境に適応させるために、既に事前学習された言語モデルと同時に新しい視覚モデルを学習することになった。しかし、Transformerはメモリを大量に必要とするため、既存の研究では、言語モデルを固定して視覚モジュールのみを学習する方法が一般的であり、クロスモーダル情報をエンド・ツー・エンドで学習する能力が制限されていた。本研究では、オーディオ・ビジュアルビデオ表現の学習において、マルチモーダルTransformerのパラメータを削減することに焦点を当てます。また、Transformerをモダリティ固有の部分とモダリティ共有の部分に分解することで、モデルが各モダリティのダイナミクスを個別に、また一緒に学習できるようにし、低ランク近似に基づいた新しいパラメータ共有スキームを提案する。また，低ランク近似に基づいた新しいパラメータ共有方式を提案し，パラメータを最大80%削減することで，モデルをエンドツーエンドでゼロから学習できることを示した．また、Transformerで学習したCNN埋め込み空間上で測定したインスタンスの類似性に基づくネガティブサンプリング手法を提案する。我々のアプローチを実証するために、Kinetics-700からの30秒のクリップで我々のモデルを事前学習し、オーディオ・ビジュアルの分類タスクに移行した。
<br>

記入欄
***

> どんなもの？<br>

<br>

記入欄
***

> 先行文献と比べてどこがすごい？

<br>

記入欄
***

> 技術や手法のキモは？

<br>

記入欄
***

> どうやって有効性を証明した？

<br>

記入欄
***

> 議論はある？

<br>

記入欄
***

> 分からない単語、調べた内容メモ

<br>

記入欄
***

> 参考リンク

<br>

記入欄
***