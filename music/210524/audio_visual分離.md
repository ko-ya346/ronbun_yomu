> タイトル<br>  

<br>

Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds  
イントゥ・ザ・ワイルド with オーディオスコープ スクリーン上の音の教師なしのオーディオ・ビジュアル分離
***

> 論文リンク<br>

<br>

https://openreview.net/forum?id=MDsQkFP1Aw
***

> 要約メモ<br>

<br>


***

> どんなもの？<br>

<br>

記入欄
***

> 先行文献と比べてどこがすごい？

<br>

記入欄
***

> 技術や手法のキモは？

<br>

記入欄
***

> どうやって有効性を証明した？

<br>

記入欄
***

> 議論はある？

<br>

記入欄
***

> 分からない単語、調べた内容メモ

<br>

記入欄
***

> 参考リンク

<br>

記入欄
***

> 本文直訳

<br>

### abstract
近年の深層学習の進歩により、音の分離や視覚的なシーンの理解に多くの進歩が見られます。しかし、自然な映像の中に明らかに存在する音源を抽出することは、依然として未解決の問題です。  
本研究では、監督なしで学習可能な新しいオーディオビジュアル音分離フレームワークであるAudioScopeを発表し、実際の自然界の動画から画面上の音源を分離する。先行するオーディオ-ビジュアル分離の研究では、音のクラスのドメインに人為的な制限を仮定し（例：音声または音楽への）、ソースの数を制約し、強力な音の分離または視覚的なセグメンテーションのラベルを必要としていました。AudioScopeは、これらの制限を克服し、音源の数が変化しても、ラベルや事前の視覚的セグメンテーションがなくても、オープンな音の領域で動作します。   
AudioScopeの学習方法は、MixIT（Mixture Invariant Training）を用いて、合成された混合音（MoM）を個々の音源に分離します。このとき、混合音のノイズの多いラベルは、教師なしのオーディオ・ビジュアル一致モデルによって提供されます。このノイズの多いラベルと、映像と音声の特徴の間の注意力を用いて、AudioScopeは、オーディオ・ビジュアルの類似性を識別し、スクリーン外の音を抑制することを学習します。  
我々は，オープンドメインのYFCC100mビデオデータから抽出したビデオクリップのデータセットを用いて，我々のアプローチの有効性を実証した．このデータセットには，制約のない状態で記録された多様な音クラスが含まれており，従来の手法の適用は困難であった．評価と半教師付き実験のために，我々はクリップの小さなサブセットについて，画面上の音と画面外の音の存在に関する人間のラベルを収集した．

### conclusion
本論文では、事前のクラスラベルや分類器に依存しない、教師なしのオープンドメインのオーディオビジュアル・オンスクリーン・セパレーション・システムをトレーニングするための最初のソリューションを提案した。我々は、人間がラベルを付けた少量の実写映像を用いて、本システムの有効性を実証した。これらのレシピは、プロジェクトのウェブページ（https://audioscope.github.io）で公開される予定です。  
将来的には、より細かい視覚的特徴、特に同期性について調査する予定です。これは、同じオブジェクトの複数のインスタンスがビデオに存在する場合に特に役立つと期待しています。また，学習した分類器を用いてYFCC100mを再フィルタリングし，画面上の音の存在を示すより良いノイズラベルを得ることも計画しており，これによりシステムの性能をさらに向上させることができると考えている．
***