> タイトル<br>

<br>

End-to-end ASR to jointly predict transcriptions and linguistic annotations  
エンド・ツー・エンドのASRで、トランスクリプションと言語アノテーションを共同で予測する
***

> 論文リンク<br>

<br>

https://www.aclweb.org/anthology/2021.naacl-main.149/
***

> 要約メモ<br>

<br>

記入欄
***

> どんなもの？<br>

<br>

TransformerベースのE2E自動音声認識を提案する。  
***

> 先行文献と比べてどこがすごい？

<br>


***

> 技術や手法のキモは？

<br>

記入欄
***

> どうやって有効性を証明した？

<br>

記入欄
***

> 議論はある？

<br>

記入欄
***

> 分からない単語、調べた内容メモ

<br>

記入欄
***

> 参考リンク
- End to Endモデル
    - 1つのNNで構成した音声認識モデルのことを指す
    - 従来の音声認識技術では、音響モデルや言語モデル、発音辞書といった複数の部品を個々に最適化して組み合わせることで音声認識システムを構築していた
    - https://techblog.yahoo.co.jp/entry/2020062930010545/

- 音素転写
    - 文字または記号を使用して音声の音を表すために使用されるシステム
    - https://www.netinbag.com/ja/business/what-is-phonemic-transcription.html
- OOV
    - out of vocabulary
- CMU
    - カーネギーメロン大学
    - 音素の辞書を公開している
    - http://www.speech.cs.cmu.edu/cgi-bin/cmudict
- CTC（Connectionist Temporal Classification）
    - 文字認識や音声認識でよく使われる手法で、LSTMやRNNと組み合わせて使用されます。文字認識や音声では一つの文字の横幅や、一つの音素の時間長さが可変です。そこで、デコーダ側で同じ文字が連続した場合に消し込むことでこの可変性の問題を解決します。
    - https://medium.com/axinc/deepspeech2-%E9%9F%B3%E5%A3%B0%E8%AA%8D%E8%AD%98%E3%82%92%E8%A1%8C%E3%81%86%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB-8288c5f53eea
- KyTea
    - 京都テキスト解析ツール
    - http://www.phontron.com/kytea/index-ja.html
- 書記素
    - 意味上の区別を可能にする最小単位（wiki）
    - 文字の最小単位
        - http://www.seiryo-u.ac.jp/u/education/gakkai/h_ronsyu_pdf/13_2/11_kouno.pdf

<br>

記入欄
***

> 本文直訳

<br>

# abstract
本研究では、自動音声認識（ASR）のためのTransformerベースのシーケンス-シーケンスモデルを提案する。  
このモデルでは、音声のトランスクリプトと、音素転写や品詞タグなどの言語情報のアノテーションを同時に行うことができる。  
言語情報は自然言語処理（NLP）において重要であるため，提案するASRは，音声対話システムや音声翻訳など，ASRとNLPを組み合わせた音声インターフェースアプリケーションに特に有効である．  
言語アノテーションを作成するために，我々は修正されたトレーニングターゲットを用いてASRシステムをトレーニングする．  
すなわち，ターゲットトランスクリプト内の各書記素または複数書記素ユニットの後に，アライメントされた音素シーケンスやPOSタグが続くようにする．  
本手法は音声データにアクセスできるため，NLPベースの手法を仮定されたASRトランスクリプトに適用するパイプラインアプローチよりも，より正確に言語アノテーションを推定することができる．  
日本語および英語のデータセットを用いた実験の結果，提案するASRシステムは，高品質のトランスクリプトと言語アノテーションを同時に生成できることがわかった．

# 1. Introduction
単一のニューラルネットワーク（NN）を使用して音声を転写するエンドツーエンド自動音声認識（E2E ASR）が最近注目されている（Graves and Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Graves, 2012; Dong et al.   
既存のE2E ASRモデルは、可能性の高い書記素、または多書記素単位を順次生成することで音声トランスクリプトを生成し、そこから言語の語彙項目を復元することができる。  
しかし、音素転写、品詞（POS）タグ、または単語境界などの他の言語的アノテーションは、根本的なオーディオ特性を理解するのに役立ちます（Simonnet et al.、2017）。  
このような言語的アノテーションは、音声対話システムなど、音声データ上で行われる自然言語処理（NLP）タスクにおいて特に重要である（Jurafsky and Martin, 2008）。  
本研究は、既存のE2E ASRモデルに、そのような言語的アノテーションを生成する能力を付与することを目的とする。  

先行研究では、E2E ASRシステムを使って複数の種類のラベルを予測することが検討されています。  
図1は、これらのシステムの図である。これらのアプローチでは、次のいずれかのモデルを使用している：一対多（O2M）モデル（Kubo and Bacchiani, 2020; Ueno et al., 2018; Gowda et al, 2019）、条件付きチェーンマッピングを用いた一対一（O2O）モデル（Shiら、2020）、または単一の配列を用いたO2Oモデル（Audhkhasiら、2018；Ghannayら、2018；Shafeyら、2019；Yadavら、2020）があります。  

図1(a)に示すO2Mモデルでは、二次ラベルの配列を推定するために余分な枝をタスクとするマルチタスク目的が用いられている。  
例えば、(Kubo and Bacchiani, 2020)では、文字情報に加えて音素情報も生成される。  
O2Mモデルは、音素性転写物と書記素性転写物を別々に生成するモデルよりも、それぞれの配列をより正確に推定することができます。  
この方法は、ベースとなるアーキテクチャに複数の損失関数を付加することで、より少ない労力で実現することができます。  
しかし、このO2Mモデルでは、音素と書記素の間の依存関係を明示的に考慮していません。  
さらに、音素と書記素のサブシーケンスを整列させるには、推論中の時間アライメントや複数のシーケンス間のアライメントに基づく追加の後処理が必要です。  
この後処理でアライメントが取れないと、ASR出力を利用した下流のNLPタスクのパフォーマンスが低下します。  

図1(b)は、条件付き連鎖マッピングを用いたO2Oモデルである。  
この複数配列モデリングの手法は、対話モデリング（Liang et al., 2020）、話者ダイアリゼーション（Fujita et al., 2020a）、複数話者ASR（Shi et al., 2020）に適用されている。  
このモデルは、O2Mモデルとは異なり、確率的連鎖法則に基づいて複数のシーケンス間の依存関係を明示的に考慮しながら、可変数の出力シーケンスを予測することができる。  
しかし、このような配列間の依存性をモデル化するには、より複雑なニューラル・アーキテクチャが必要であり、配列のアラインメントは推論時に後処理が必要である。  

O2Oモデルを使用するもう1つの選択肢は、図1（c）に示すように、条件付きチェーンマッピングを使用する代わりに、複数の配列を1つの配列として出力することです。  
例えば、(Audhkhasi et al., 2018)では、O2Oモデルは、最初に単語の構成文字を生成し、続いて単語そのものを生成することで、単語のトランスクリプトを生成している。  
また、(Shafey et al., 2019)で検討された別のアプリケーションでは、O2Oモデルを使用して、書記素に続いて話者の役割を生成しました。  
このアプローチは、一次配列から配列へのマッピングを生成するために使用されたニューラルネットワークアーキテクチャを、二次ラベル配列を生成するために再利用できるため、実装が最も簡単である（例えば、コネクショニスト時間的分類（CTC）ベースのシステムなど）。  
前述の2つのアプローチとは対照的に、O2Oモデルでは、出力シーケンスが単語と対応するアノテーションラベルの間のアライメントを保持するため、推論時にラベルシーケンスをアライメントするための後処理を必要としません。  
アライメントが必要なのは、適切なターゲットシーケンスを生成するための学習時のデータ準備段階のみです。  
このような理由から，本研究ではO2Oモデルを使用した。  

本論文では、頻繁に支持されているCTCベースのアプローチ（Audhkhasi et al., 2018; Ghannay et al., 2018）ではなく、1つのシーケンスを持つO2Oモデルに対して、最先端のTransformerベースのE2E ASRシステム（Karita et al., 2019）を使用することを提案する。  
CTCベースのシステムと比較して、このアプローチは、図1（b）の条件付き連鎖規則モデルと同様に、自己回帰デコーダネットワークのおかげで、出力ラベル間の関係を明示的にモデル化することができます。  
また、CTCベースのシステムと比較して、性能が向上していることを示しています。   
もう一つの貢献は、本手法の有用性を分析・実証するために、広範な実証的評価を行ったことです。  
例えば、この手法を、音声トランスクリプトとPOSタグが同時に生成される英語と日本語のASRタスクに適用しました。  
本手法は、対応する文字列が間違っていても、言語アノテーションを正しく予測します。  
一方、仮説的なASRトランスクリプトにNLPベースの手法を適用するパイプラインアプローチでは失敗します。  
この特徴は、スロットフィリングや意図検出などの下流のNLPシステムに役立ちます。  
その上、E2Eモデルはスモールフットプリントの予測をアーカイブするため、我々のアプローチはオンデバイスのアプリケーションに適しています（Pang et al.   
なお、我々の主な目標は、ASR性能の劣化を最小限に抑えて、アライメントされたトランスクリプトと言語的アノテーションを提供することです。  
我々はASR性能の向上を目指していない。提案手法の特徴を以下にまとめます。  

- 提案するTransformerベースのO2Oモデルは、O2MやCTCベースのO2Oモデルとは異なり、出力された書記素と対応する言語アノテーションとの関係を明示的にモデル化することができる。
- 我々のアプローチでは、推論の際に、転写と言語的注釈の順序にまたがる追加のアライメントの後処理を必要としません。
- 提案するO2Oモデルを下流のNLPタスクと簡単に組み合わせることができ、また、直観的なエラー分析（例えば、単語と対応する音素の出力をチェックすることで、同音異義語に起因するエラーを検出する）を行うことができます。

# 2 Existing E2E ASR system
## 2.1 E2E ASR
E2E ASRの目的は，入力特徴列X = {xi∈\<Din>}L m=1から，出力トークン列y = {ym∈Y}L m=1を推定することである。  
ここで，DinとI inはそれぞれ入力特徴の次元数と入力配列の長さを表し，LとYは出力配列の長さとトークンの語彙を表す。  
出力トークン列を予測するために，NNは以下の条件付き尤度目的関数を最大化するように学習される。  
ランタイム中、ASRの出力yˆは次のように予測されます。  
ここで，Y ∗ はすべての可能な仮説の集合を表します。  

Transformer（Vaswani et al., 2017）は、最先端のNNアーキテクチャであり、式（1）を最大化するために使用することができます。  
Transformerは、2つのNNで構成されています。  

1. エンコーダーネットワークとデコーダーネットワーク  
2. IembとDembを、音響埋め込みのシーケンス長と次元とする  

Encoderネットワークは、入力された特徴量列から、音響情報の埋め込み列E = {ei ∈\<Demb} Iemb i=1、すなわちE = Encoder(X)を生成する。  
Decoderネットワークは，現在の出力y¯ = {y1, - - , yM-1}とEを含むサブシーケンスが与えられたときに，M番目のステップyMの出力を予測する，すなわちyM = Decoder(y¯, E)．  
この条件付き自己回帰モデリング関数は、CTCとは異なり、出力ラベル間の関係を明示的にモデル化できるため、本稿では特に重要である。

## 2.2 E2E ASR to predict two or more sequences
本研究では、単語・モーフェム配列と、音素転写やPOSタグなどの言語アノテーションを同時に推定することを目的としています。  
ここでは，セクション2.1のサブワードシーケンスy1と追加の言語アノテーションシーケンスを含むシーケンスの数をy2, ... ... , yKと定義する。, yKとする。  
音写と言語アノテーションの両方を予測するために，NNは以下の合同確率の対数尤度を最大化するように学習される。  

ここで、yk = {y1,k, - - , yMk,k|ym,k∈Yk}は、k番目のタイプのトークンまたは言語的注釈のMk長のシーケンスを示し、Ykは対応するトークンまたはシンボルのセットを示す。  
本節では、式(3)を最大化するための以下の既存モデルについて説明する。  
(3)を最大化する既存のモデルとして，マルチタスク学習を用いて学習したO2Mモデルと，条件付き連鎖写像を用いて学習したO2Oモデルについて説明する．

### 2.2.1 O2M model trained with multi-task learning
式(3)を最大化するNNアーキテクチャ(Ueno et al., 2018; Gowda et al., 2019; Sanabria and Metze, 2018; Adams et al., 2019)として頻繁に用いられるのが、マルチタスク学習で学習したO2Mモデルである。  
図1（a）にモデルのアーキテクチャを示す。  
O2Mモデルは、複数種類の配列を独立して出力する。  
つまり、マルチタスク学習は、Eq.3に対して出力トークンの種類の条件付き独立性を仮定することで、以下のように導かれる。  
ここで、y¯1:m-1,k = {y1,k, - - , ym-1,k}は、m - 1までのk番目のタイプのトークンまたは言語的アノテーションのサブシーケンスを示す。  
式(4)のラインクロス部分は、条件付き独立性を仮定することで、シーケンスy1, - - - , yk-1が無視されることを表している。  
本研究の目的は、単語・形態素とアライメントされた言語アノテーションを共同で予測することである。  
O2Mモデルでは、異なる長さの配列を扱うため、複数の配列を揃えるための後処理が必要となる。  
また、Eq. (4)は、マルチタスク学習が条件付き独立性を仮定していることを示しているが、トランスクリプトと言語アノテーションは条件付きで依存していることが多い。  
したがって、O2Mモデルはこの研究には理想的ではありません。

### 2.2.2 O2O model trained with conditional chain mapping
また，条件付きチェーンマッピングで学習したO2Oモデル（Fujita et al., 2020a; Shi et al. (3). 図1(b)にこのモデルのアーキテクチャを示す。  
このモデルは、異なるシーケンスタイプを順次予測し、その都度、以前にデコードされたすべてのシーケンスタイプ1 ... ... k - 1を条件とする。  
O2O条件付き連鎖写像モデルは，O2Mモデルで用いられているマルチタスク学習損失（式（4））とは異なり，確率的連鎖法則の再帰的展開により，結合対数尤度（式（3））を最大化するように学習されます．  
このモデルは，配列タイプ間の条件付き独立性を必要とせず，仮定もしません．  
形式的には，O2Oモデルは，以下の損失関数を最大化するように学習されます．  

ここで，Y¯ 1:k-1 = {y1, - - , yk-1} は (k - 1) 個の配列を表します．  
この方法では、配列間の依存関係を明示的にモデル化できますが、出力配列を揃えるための後処理が必要です。

# 3 Proposed E2E ASR system
## 3.1 Framework
図1(c)は，提案する単一配列O2O E2E ASRモデルを示している．  
単一配列O2Oモデルは，複数の配列を1つの配列とみなして，単語/コーフェムと，それに対応する言語アノテーションを同時に予測する．  

単一シーケンス表現では、K個の出力シーケンスがS個のセグメントの単一シーケンスに折り畳まれます。  
i番目のセグメントsiは、固定順序のK個の共同整列したサブシーケンスから構成されます。y¯i,kをインデックスB(i,k)からE(i,k)までのk番目のタイプのトークンまたはアノテーションのi番目のサブシーケンスとすると、すなわち、y¯i,k := y¯B(i,k):E(i,k),kとする。  
そして、si = (y¯i,1, ... , y¯i,K)は、アライメントされた書記素と言語アノテーションのサブシーケンスで構成されるi番目の可変長セグメントを示す。  
式8は、K個のシーケンスがセグメントsiで構成される単一のシーケンスに折り畳まれる様子を示しています。  
si を得るために，既存のアノテーションツールまたは手動アノテーションを用いて，K 個の出力配列タイプの学習セットを共同でアラインメントします．  
これらのセグメントは，自動回帰予測タスクの学習ターゲットとして使用されます．  
このようにして，我々のモデルは，入力XからK個の出力配列を同時に予測・整列することを暗黙的に学習する．  

ここで，y ∗ i を折り畳まれた単一配列表現 s1, ... ... , sS の要素とすると，共同対数尤度（式（3））は次のように書ける．  
sS とすると，共同対数尤度（式（3））は次のように書ける．  
この形式は、変数y∗ mがK個の出力シーケンスを表すK個のシンボルセットの組合わせから値を取ることと、このシーケンスの長さM∗ = PK k=1 Mk, がK個の出力シーケンスの長さの合計であることを除けば、式(1)の単一シーケンスの目的関数とほぼ同等であることに注意してください。  

本フレームワークは，セクション2で説明した既存のフレームワークと比較して，様々な利点があります。  
2.2.2節の条件付き連鎖写像で学習したO2Oモデルと同様に，本フレームワークは出力ラベル間の条件付き独立性を仮定せず，単語・形態素と言語アノテーション間の依存関係を柔軟にモデル化することができる。  
関連する作品としては、O2Oモデルを用いたものがありますが、(Yadav et al., 2020)などがありますが、これらはCTCに基づいており、このような明示的な出力依存性を考慮していません。  
また、Transformerを用いた提案手法は、式(8)のアラインド表現siに基づいて、シーケンス全体にわたって、単語/コーフェムと対応する言語アノテーションの関係を保持することができる。  
最後に、このフレームワークは元々の単一配列の目的関数と同等であり、アルゴリズムを変更することなく、既存の強い配列間モデル（本論文ではTransformer）を使用することができます。  
唯一のプロセスは、次のセクションで説明するsiからなる崩壊した単一配列を準備することである。

## 3.2 Data preparation
本節では、式(8)のsiで構成される折りたたまれた単一配列を準備する方法を説明する。  
英語（TED-LIUM release 2 (TEDLIUM2) (Rousseau et al., 2014)）と日本語（corpus of spontaneous Japanese (CSJ) (Maekawa et al., 2000)）のデータを例に挙げて、このデータ準備を説明する。  
シーケンスタイプには、書記素と音素のトランスクリプト1に加えて、POSタグ  

図2は、ターゲット配列の取得方法を示したものである。  
まず、人手によるアノテーションラベルやアノテーションツールを用いて、文字列から音素やPOSタグの配列を予測する（図2(a),(b)）。  
日本語のデータでは、コーパスに含まれるアノテーションラベルを使用している。  
なお，POSタグの一部は形態素解析モデルを用いて推定したものである。  
英語のデータでは，コーパスに含まれる発音辞書とWordNet (Miller, 1998)からこれらの配列を取得しています。  
語彙の中には、発音辞書に2つ以上の発音があるものがあります。  
音素配列を得るために、発音候補の中から単語ごとに1つの発音をランダムに選びました。  
WordNetでは57%の単語にPOSタグが付与されていないため、これらのラベルにPOSタグシステム(Loper and Bird, 2002)の出力を付与しました。  
次に，これらの音素とPOSタグを特殊な記号に置き換えて（図2（c）），書記素記号と区別しました。  
第3に、単語の境界で書記素と言語のアノテーションシーケンスを分割し、サブシーケンス（式（8）のy¯i,k）を得る（図2（d））。  
次に、サブシーケンスをセグメント（式（8）のsi）に集約し、式（8）の方法でターゲットシーケンスに折り畳む（図2（e））。  
英語データの場合、折り畳まれたターゲット配列にBPE（byte-pair encoding）（Kudo and Richardson, 2018）を適用した（図2（f））。

# 4 Experiments
## 4.1 Experimental setup
### 4.1.1 E2E ASR
ESPnetツールキットを用いて、TransformerベースのASRシステムを構築しました（Watanabe et al, 2018）。  
Transformerのアーキテクチャとトレーニング/デコーディング用のハイパーパラメータは、ESPnetの既存のレシピに基づいています。  
self-attentionベースのCTC（Pham et al., 2019）、Transformer（Dong et al., 2018）、および補助的なCTC目的で訓練されたハイブリッドTransformer（Transformer+CTC）（Karita et al., 2019）の3つのモデルを調査しました。  
CTCモデルは、O2Oモデルに基づく先行研究で使用されたもので、例えば(Audhkhasi et al., 2018; Yadav et al., 2020)があります。  
トレーニング中、CTCモデルは、Transformer+CTCと同様のマルチタスク学習方式でTransformerデコーダで正則化された。  
このような正則化技術は、純粋なCTCベースラインよりも大幅な改善をもたらす（Fujita et al.  

Transformer+CTCの学習については、パフォーマンスを向上させるためにジョイントCTCトレーニングを適用した（Karita et al.2019）。  
CTCベースのデコーディングでは、 greedy search algorithm を使用しました。Transformerのデコーディングでは、ビーム探索アルゴリズムを使用し、開発セットを使用して探索パラメータを調整しました。  
Transformer+CTCモデルについては、Transformer/CTC joint decoding (Karita et al., 2019).を適用し、開発セットを用いて目的語の重みを調整した。  
なお、言語モデルの浅い融合（Hori et al., 2018）は、予備実験で効果が見出せなかったため、適用していません。

### 4.1.2 Evaluation criteria
提案手法の性能を，文字誤り率（CER），音素誤り率（PER），単語誤り率（WER）を用いて評価した。  
CERとWERは，それぞれ日本語と英語の書記素の品質を測定します．  
CERとWERは，それぞれ日本語と英語の文字表記の品質を測定し，PERは，両言語の音素表記の品質を評価するために用いられる。  
本研究では、最新のTransformerベースのE2E ASRに言語アノテーション予測を組み込むことを目的としています。  
CER/WER/PERを計算し、下流のNLPタスクが追加されてもE2EモデルがASRを十分に実行できることを検証した。  

推論段階でアラインメント（式（8）のsi）を持つ配列を得るためには、学習段階と同様に、書記素、音素、POSの順に生成される必要がある。  
これを確認するために、アノテーション構造精度（ASA）を指標として定義する。  
予測された構造の正しい数を計算し、精度を算出することができる。  
例えば，出力の正しい順序は，以下の graphem-phonem-POS の順序に従っている必要があります．  
> \<s> I \<Ph12> \<Pos3> go \<Ph21> \<Pos5> \</s>   

ここで、\<s>と\</s>はそれぞれ文の開始記号と終了記号を表しています。  
しかし、我々の配列対配列モデルは、このような明示的な出力制約を持たないため、おそらく以下のような誤った順序の配列を出力してしまいます。  
> \<s> I \<Ph12> \<Pos3> go \<Pos5> \</s>   

したがって、2番目のケースでは、6つのトランジションカウントのうち、5つの正しいトランジションカウントがあり、精度は5/6と計算できる。  "go"から「\<Pos5>」への遷移が正しくないと仮定します。

日本語ASRの単語分割性能を評価するために、仮説分割の精度p、リコールr、F値fを、グランドトゥルース分割と比較して測定する。  
Nhyp, Nref, Ncorをそれぞれ、予測された文字列、参照文字列、予測された言語的注釈が正しい文字列の数とする。  
p = Ncor/Nhyp , r = Ncor/Nref , f = 2pr/(p + r)と定義した．  
ASRエラーの影響を無視するために，参照用と仮説用のトランスクリプトが完全に一致する1,919の発話のみを比較した.  

さらに、仮説のASRトランスクリプトと参照トランスクリプトを書記素に合わせ、言語的アノテーションの性能を測るためにアノテーション精度を計算した。  
推定された書記素が正しい入力単語の数と、推定された書記素と言語的アノテーションが正しい単語の数をそれぞれNinとNcorとする。  
精度はNcor/Ninで計算される。  
アノテーション精度を計算する際には、ASRによって誤った書記素が予測された単語は扱わないので、アノテーション精度はASRの誤差に対してロバストになります。  

上記の単語分割と言語アノテーションの指標ではASRエラーが考慮されていないため，最終的に，すべての発話（つまりASRエラーを含む）を用いて，正規化編集距離，精度，リコール，F値を計算しました。

### 4.1.3 Baseline of the linguistic annotation
言語アノテーションの性能を比較するために、我々はパイプラインシステムを用意しました。  
パイプラインシステムでは、まず、CTC+Transformerの分離モデルが書記素配列を予測する。  
次に、KyTea (Graham and Mori, 2010)を用いて学習したL2正規化付きの線形SVMが、予測された配列から単語の境界と言語的アノテーションを予測する。  
KyTeaの学習には、提案手法との公平な比較を行うために、ASRトレーニングセットの転写文字のみを使用した。  

日本語タスクのパイプラインシステムでは、言語的アノテーションを予測する前に、単語のセグメンテーションが必要です。  
一方、提案するASRは、単語の分割と言語的アノテーションを同時に実現します。  
また、提案するASRでは、文字情報と音響情報を用いてこれらの推定を行うが、パイプラインシステムでは文字情報のみを用いている。  
そのため、提案手法は、書記素情報だけでは推定が困難な文の単語境界と言語的注釈をより適切に予測できると期待される。  
その上、我々のモデルは、ASRの転写が誤って予測されていても、言語的注釈を正しく予測する可能性がある。  
一方、パイプラインアプローチは、仮説的なASR転写にASRエラーが含まれている場合、言語的注釈の予測に失敗する。  
これは、スロットフィリングや意図検出のような下流のNLPベースのシステムに役立ちます。

## 4.2 Performance of speech recognition
> 表1
CTCモデル、Transformerモデル、Transformer+CTCモデルの比較。  
CER、PER、WERはそれぞれ、文字エラー率、音素エラー率、単語エラー率を示す。  
CTCモデルは先行研究で広く使われている（例えば、Ghannayら、2018）。  
Transformer+CTCは、CTCのトレーニングとデコーディングを共同で行うTransformerを指す（Watanabe et al.

> 図3
分離モデルと提案されたジョイントモデルの比較。  
分離モデルでは書記素または音素を予測し、ジョイントモデルでは書記素と言語アノテーションを同時に予測する

我々は、提案手法が高品質の転写および言語的アノテーションを生成できることを確認するために、ASR性能を評価した。  
なお，我々の第一の目標は，十分な性能を維持しながら転写と言語アノテーションを同時に予測することであり，ASR性能そのものを向上させることではない．  
表1と図3は，日本語（CSJ）と英語（TEDLIUM2）のタスクにおけるASR性能を示している．  

まず、どのモデルアーキテクチャが書記素と音素の配列を予測するのに適しているかを議論します。  
表1は、TransformerまたはTransfromer+CTCが、従来の手法に相当するCTCモデルと比較して優れた性能を達成していることを示しています。  
これは、Transformerが、第3節で述べたように、明示的な依存関係のモデル化により、CTCよりも転写および言語的アノテーション（本実験では音素）の予測に優れていることを意味します。  
Transformer+CTCはTransformerよりも優れた、あるいは同等の性能を得ることができるため、本稿以降ではTransformer+CTCアーキテクチャをベースモデルとして使用しています（ジョイントモデルと呼びます）。   

第二に，提案した共同モデルが十分な性能で書記素と音素を予測するかどうかを議論する．  
これを確認するために、文字列または音素列を予測する2つの別々のモデルを学習しました。  
Transformer+CTCは、CTCモデルとTransformerよりも優れた性能を発揮するため、Transformer+CTCアーキテクチャをベースモデルとして使用した。  
図3は、提案したジョイントモデルが、特に書記素と音素の両方を予測する場合に、分離モデルとほぼ同等の性能を発揮することを示しています。  
ジョイントモデルの予測にPOSタグが含まれる場合、特に日本語タスクでは若干の劣化が見られました。  
しかし、このような劣化はまだ1%未満であり、Transformer+CTCの提案するO2Oモデルは、十分な性能で書記素と音素を同時に予測することができると結論づけることができる。  
強調したいのは、従来の分離モデルではできなかった、書記素／音素／POS間のアラインメントが、提案した共同モデルでは可能だということです。  

## 4.3 Performance of the annotation structure prediction
4.1.2節で述べたように，アノテーション構造精度(ASA)を計算したところ，その範囲は98.9%から100.0%であることがわかりました。  
これは、提案したジョイントモデルが、転写と言語的アノテーションをほぼ完全に正しい順序で一貫して予測できることを意味しています。  
遷移のエラーのほとんどは最後の単語で発生しており、これはビームサーチのエラーが原因である可能性があることがわかりました。  


## 4.4 Performance of word segmentation and predicting linguistic annotations
> 図4
推定された転記の例。X/Y/ZのXは書記素、Y、Zはそれぞれ音素とPOSタグを表す  

> 表2
CSJにおける単語分割のパフォーマンス(%)。  
パイプラインは、ASRの予測転写に続いて、NLPベースの言語アノテーションシステム(Graham and Mori, 2010)を使用しています。  
提案されたものは、音声から書記素と音素を予測し、続いてPOSタグを予測する。  
なお、評価には、仮説に基づくASR転写が正しく予測された文のみを用いた。

> 表3
言語アノテーションの予測精度(%)。  
なお、ASRエラーの影響を無視するために、リファレンスに登場しない文字列を評価から外した。

> 表4
評価セット全体で平均化された正規化編集距離。  
ASRエラーの影響を考慮していることに注意。



提案したASRの出力（書記素，音素，POSタグを予測する）を用いて，単語分割と言語アノテーションの性能を評価した．  
なお，英語の文章には単語の境界線が含まれているため，英語タスクでは単語分割の性能を計算しなかった．  

表2と表3は、それぞれ、単語のセグメンテーションと、予測言語アノテーションの性能を示しています。  
これらの結果は、ASRのエラーを考慮していないことに注意してください。  
これらの表から、提案したASRシステムは、パイプラインシステムよりも優れた単語分割を達成し、言語アノテーションを予測していることがわかる。  
CSJでは41k、TEDLIUM2では65kの形態素を使用していますが、このサンプル数は、我々のモデルがパイプラインアプローチよりも優れていることを示すのに十分であると考えられます。  

表4と表5は，ASRのエラーを考慮したパフォーマンスを示している．  
表4によると，提案したASRシステムはパイプラインに比べてPOSタグの予測に優れているが，提案システムが転写の予測に失敗することもあった2．  
また、表5では、日本語タスクにおいて、提案したASRシステムがより良いパフォーマンスを予測することが確認された。  
英語タスクでは、パイプラインシステムと提案型ASRシステムの性能は同等ですが、提案型ASRは、下流のNLPタスクを追加するための余分なメモリを必要としない点を強調したいと思います。  
これは、スモールフットプリントのシステムを開発するのに有効である.  

図4は，提案するASRが単語の境界や音素を正しく推定できることを示す例である．  
例えば、第1文では、音響情報から推定した "repetition "というPOSタグに基づいて、単語の境界を正しく分割しています。  
同様に、第2文では、音響情報から正しい発音を適切に選択している。




# 5. Discussion
## 5.1 Building pronunciation dictionary

> 表6注釈  
提案されたE2E ASRの出力を用いて生成された発音辞書のエントリ。  
X/YのXとYは，それぞれ転写と音素配列を表し，REFとHYPはそれぞれ参照と仮説を表す。  
(a)の行と(b)の行には、それぞれ、語彙力のない単語のエントリと異名のエントリがリストアップされている。

我々のシステムであるE2E ASRは、各単語の書記素と音素のペアを推定することができるので、書記素から音素への配列と音響情報の両方を考慮することで、発音辞書を構築することができます。  

表6は，我々のシステムの出力から抽出した発音辞書の項目を示している．  
表の最初の行には、学習セットのテキストに現れなかった単語で、音素配列が正しく推定されたエントリがリストアップされています。  
これらのエントリは、我々のシステムがOOVの単語の音素を予測できることを示しています。  
表の2行目は、音素がリファレンスとは異なるが、CMUの発音辞書(CMU)に存在するエントリを示しています。  
つまり、これらのエントリには、各単語の音素配列のバリエーションがあり、その音素配列が正しく予測されています。  
本研究では、学習セットから、各書記素の音素配列のバリエーションを削除しました。  
言語情報のみを用いて音素配列を予測するようにTransformerを学習した場合、音素配列は決定論的に単語にマッピングされてしまう可能性があります。  
興味深いことに、我々のTransformerは、各単語の音素配列のバリエーションを復元します。  
これは、音素配列の予測に音響情報が寄与していることを意味します。

## 5.2 Attension pattern
> 図5
Decoderネットワークの第3層の注目パターンの例。  
変形器は書記素と音素を同時に推定する。


Transformerのもう一つの利点は、自己注意とソース・ターゲット注意の重みのパターンを視覚化することで、Transformerの内部で何が起こっているかを推測できることです。  

図5は、Decoderネットワークの第3層における自己注目度とソース・ターゲット注目度のパターンを示しています。  
この図では、自己注意は単調に変化していますが、さらに斜めの点線が入っています。  
これは、自己注意が複数の（書記素と音素の両方の）出力シンボルを使用しているが、ほとんど順序を維持していることを意味している。  
同様に、ソース・ターゲット注意が、同じタイムステップの音響特徴に2回焦点を当てていることも示しています。  
これは、書記素と音素の両方が、それぞれ同じ時間ステップの同じ音響特徴を用いて予測されることを示しています。  

# 6. conclusion and feature work
我々は、音素転写やPOSタグなどの言語アノテーションを同時に推定する新しいE2E ASR Transformerシステムを提案した。  
本論文では、提案したASRがこれらの特徴を十分な性能で推定できることを示し、また、両方の出力シンボルの出力を揃えたおかげで、合理的な音素・書記素の分析と注目パターンを示すことができた。  
今後は、提案手法を拡張して、名前付きエンティティなどの他の言語アノテーションを予測する予定である。


***
